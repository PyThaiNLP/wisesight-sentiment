# Wisesight Samples with Word Tokenization Label

This directory contains samples of Thai social media text, tokenized by humans. These samples are randomly drawn from the full [Wisesight Sentiment Corpus](https://github.com/PyThaiNLP/wisesight-sentiment).

For `wisesight-160`, we drawn 40 samples for each label. And 250 samples each for `wisesight-1000`. 

**Remark:** We removed a couple of samples from wiseight-1000 because they look like spam.

Althought we have two sets of data, we recommend to use **wisesight-1000** because it contains more samples.
Hence, its evaluation is more respresentative and reliable.

Because these samples are representative of real world content, we believe having these annotaed samples will allow the community to robustly evaluate tokenization algorithms.

## Acknowledgement

The annotation was done by several people, including Nitchakarn Chantarapratin, [Pattarawat Chormai][pc], [Ponrawee Prasertsom][pp], [Jitkapat Sawatphol][js], [Nozomi Yamada][ny], and [Attapol Rutherford][ar].

[pc]: https://github.com/heytitle
[pp]: https://github.com/ponrawee
[js]: https://github.com/jitkapat
[ny]: https://github.com/nozomiyamada
[ar]: https://attapol.github.io/
